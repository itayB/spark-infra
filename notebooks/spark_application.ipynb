{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta, time\n",
    "\n",
    "\n",
    "#   .set(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"false\") \\\n",
    "#.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark-ml-executor\") \\\n",
    "\n",
    "#       .set(\"spark.driver.memory\", \"1G\") \\\n",
    "#       .set(\"spark.driver.extraClassPath\", \"/opt/conda/lib/python3.8/site-packages/pyspark/jars/hadoop-aws-3.2.0.jar:/opt/conda/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-bundle-1.11.888.jar\") \\\n",
    "#       .set(\"spark.executor.extraClassPath\", \"/opt/conda/lib/python3.8/site-packages/pyspark/jars/hadoop-aws-3.2.0.jar:/opt/conda/lib/python3.8/site-packages/pyspark/jars/aws-java-sdk-bundle-1.11.888.jar\") \\\n",
    "#       .set(\"spark.kubernetes.executor.podTemplateFile\", \"/podTemplate.yml\") \\\n",
    "\n",
    "\n",
    "\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local\")\n",
    "    conf \\\n",
    "      .set(\"spark.kubernetes.namespace\", \"spark-ml\") \\\n",
    "      .set(\"spark.kubernetes.container.image\", \"itayb/spark:3.1.1-hadoop-3.2.0-aws\") \\\n",
    "      .set(\"spark.executor.instances\", \"2\") \\\n",
    "      .set(\"spark.executor.memory\", \"2g\") \\\n",
    "      .set(\"spark.executor.cores\", \"2\") \\\n",
    "      .set(\"spark.driver.port\", \"2222\") \\\n",
    "      .set(\"spark.driver.blockManager.port\", \"7777\") \\\n",
    "      .set(\"spark.driver.host\", \"jupyter.recs.svc.cluster.local\") \\\n",
    "      .set(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "      .set(\"spark.ui.port\", \"4040\") \\\n",
    "      .set(\"spark.network.timeout\", \"240\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.endpoint\", \"localstack.kube-system.svc.cluster.local:4566\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "      .set(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
    "      .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "    # print(swan_spark_conf.toDebugString()) #Instance of SparkConf with options set by the extension\n",
    "\n",
    "    return SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "\n",
    "def dates_list(start_date, end_date):\n",
    "    start_date = datetime.combine(start_date, time())\n",
    "    end_date = datetime.combine(end_date, time())\n",
    "    if start_date <= end_date:\n",
    "        return [(start_date + timedelta(days=x)).strftime(\"%Y-%m-%d\") for x in range(0, (end_date-start_date).days + 1)]\n",
    "    return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-arkansas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
